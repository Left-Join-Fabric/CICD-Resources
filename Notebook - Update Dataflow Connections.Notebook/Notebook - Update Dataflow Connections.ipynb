{"cells":[{"cell_type":"markdown","source":["# Fabric CI/CD:  Update Dataflow connections/destinations for new environment deployments\n","\n","Background: When Dataflows (Gen2) are migrated to a new workspace, if the query destination is a Lakehouse in the original workspace then it will not automatically update and use the new Lakehouse in the new workspace.\n","\n","This script solves that problem by looking for old connection information and replacing it the new version, using the Fabric REST API.\n","It will update both the Default Destination and any individual query destinations,\n","    **as long as the current destination matches the source workspace/lakehouse provided below.**\n","\n","Inputs: \n","\n","- target_workspace:   The workspace containing the Dataflows to update\n","- target_lakehouse:   A Lakehouse name, in the target workspace, The name of the new Lakehouse, which will be used to set the new connection\n","- source_lakehouse:   The name of the Lakehouse currently used as the query destination\n","- source_workspace:   The name of workspace containing the source_lakehouse\n","\n","**All Dataflows in the target workspace are examined, all references to the source workspace or lakehouse are updated to the target info.**\n","\n","Connections in Fabric require both the Workspace and Lakehouse ID.\n","\n","Assumption:  The new destination is a Lakehouse in the same workspace as the migrated Dataflow.\n","\n","\n","\n","\n"],"metadata":{},"id":"2d3fdd5a-fbd7-468c-8e41-2acc24e1b114"},{"cell_type":"code","source":["# Input parameters.  Modify any of the variables in this cell prior to running\n","\n","target_lakehouse='Lakehouse_Silver'\n","source_lakehouse=target_lakehouse\n","\n","source_workspace= 'Data Integration'\n","target_workspace= 'Data Integration (Test)'\n","description_comment = 'Destination set to ' + target_workspace\n","\n","FABRIC_API = \"https://api.fabric.microsoft.com/v1\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"80f30fa7-ea98-49ea-9df0-bd1466d3077c"},{"cell_type":"code","source":["#Import libraries\n","\n","import sempy.fabric as fabric\n","import pandas as pd\n","import base64\n","import json"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d045745e-fb8f-4616-89d0-4d4dcaf44c2d"},{"cell_type":"markdown","source":["### Define functions"],"metadata":{},"id":"b34e7c3d-b80a-4613-ad64-ef7f876c0224"},{"cell_type":"code","source":["def get_workspace_items(workspace_name):\n","\n","    workspace_id = fabric.resolve_workspace_id(workspace_name)\n","\n","    if len(workspace_id) < 10:\n","        print(len(df_selected_workspace))\n","        raise RuntimeError(\"Invalid Workspace Name\")\n","\n","    else:\n","\n","        url = f\"{FABRIC_API}/workspaces/{workspace_id}/items\"\n","        response = client.get(url)\n","    \n","        return workspace_id,pd.json_normalize(response.json()['value'])"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"66109246-b6f7-4a37-9a1c-e5b5550764d7"},{"cell_type":"code","source":["def update_item_property(item_type,item_id,property_name,new_value):\n","\n","    payload={f'{property_name}': f'{new_value}'}\n","    url = f\"{FABRIC_API}/workspaces/{target_workspace_id}/{item_type}/{item_id}\"\n","  \n","    response = client.patch(url, json=payload)\n","    \n","    return response.status_code"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"77f89e4a-6b35-4ea9-82e3-1ac38438f72b"},{"cell_type":"code","source":["def update_dataflow_destination(item_to_update,workspace_id,source_workspace_id,target_workspace_id,source_lakehouse_id,target_lakehouse_id):\n","\n","    url = f\"{FABRIC_API}/workspaces/{workspace_id}/dataflows/{item_to_update}/getDefinition\"\n","    response = client.post(url)\n","\n","    if response.status_code != 200:\n","        raise RuntimeError(f\"Unable to retrieve item.  Response Status Code: {response.status_code}\")\n","\n","    previous_definition=response.json()\n","\n","    # Index 1 contains the query Steps and Destination of a Dataflow\n","    payload_64 = previous_definition[\"definition\"][\"parts\"][1][\"payload\"]\n","\n","    # Decode the payload to plain text\n","    decoded_bytes=base64.b64decode(payload_64)\n","    old_payload_text=decoded_bytes.decode(\"utf-8\")\n","    \n","    # The query definition is now plain text and can be modified as a string\n","    #  Replace the previous lakehouse connection values with the new values\n","    new_payload_text=old_payload_text.replace(source_workspace_id,target_workspace_id)\n","    new_payload_text=new_payload_text.replace(source_lakehouse_id,target_lakehouse_id)\n","\n","    if new_payload_text==old_payload_text:\n","        return 0  # No update is necessary, exit the function\n","    \n","    # Convert the payload string back to Base64\n","    encoded_bytes=base64.b64encode(new_payload_text.encode('utf-8'))\n","    encoded_string=encoded_bytes.decode('utf-8')\n","\n","    # Replace the old payload with the new payload, keeping the rest of the definition as is\n","    updated_definition=previous_definition\n","    updated_definition[\"definition\"][\"parts\"][1][\"payload\"]=encoded_string\n","\n","    # The updated definition is now complete, use the API to update the definition\n","    url = f\"{FABRIC_API}/workspaces/{workspace_id}/dataflows/{item_to_update}/updateDefinition\"\n","    response=client.post(url,json=updated_definition)\n","  \n","    return response.status_code\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"40fbf607-2273-40f9-9357-145fb67becca"},{"cell_type":"code","source":["def append_item_description(item_type,item_id,description_comment):\n","\n","    # Get the old description\n","    url = f\"{FABRIC_API}/workspaces/{target_workspace_id}/{item_type}/{item_id}\"\n","    response = client.get(url)\n","\n","    if response.status_code != 200:\n","        raise RuntimeError(f\"Unable to retrieve description for item id: {item_id}.  Response Status Code: {response.status_code}\")\n","    else:\n","        old_description=response.json()[\"description\"]\n","        #print(old_description)\n","\n","        # Appened the new description comment to the old comment\n","        # If the dataflow has been updated previously, make sure and account for this without duplicating the comment\n","        temp = description_comment.replace(target_workspace,'')\n","\n","        pos = old_description.find(temp)\n","        if pos>=0:\n","            old_description = old_description[:pos]\n","\n","        new_description = old_description + \"\"\"\n","\n","\"\"\" + description_comment\n","\n","        return new_description"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10aea2bd-77e6-48d5-92ec-6ae58effd0b0"},{"cell_type":"markdown","source":["# Code Section: Update the Connection properties in the Dataflows"],"metadata":{},"id":"96a8f80f-37f2-4614-bc95-5cc6e8bfc1be"},{"cell_type":"markdown","source":["**Process Overview**\n","\n","1. Based on the inputs provided, use the Fabric REST API to lookup up ID values of the workspace/lakehouse\n","2. Retrieve a list of all dataflows in the target workspace and loop through it\n","3. If the data flow references the source connection, replace it with the target connection\n","4. Record the update by updating the dataflow description"],"metadata":{},"id":"abd879b1-3b8a-4a96-8ea7-dfa0c2f40b40"},{"cell_type":"markdown","source":["### Find the items to update along with the new and old connection info"],"metadata":{},"id":"3315f858-513a-47a8-a20f-e4e9ee4ed2a1"},{"cell_type":"code","source":["#Instantiate the client\n","client = fabric.FabricRestClient()\n","\n","# Get a list of available workspaces\n","#url = f\"{FABRIC_API}/workspaces\"\n","#response = client.get(url)\n","#df_workspaces = pd.json_normalize(response. json()['value'])\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"34029e8e-3244-4644-825f-dc020c738fe5"},{"cell_type":"code","source":["# The custom get_workspace_items returns a workspace id and a dataframe of all items within that workspace\n","\n","source_workspace_id,df_source_items = get_workspace_items(source_workspace)\n","target_workspace_id,df_target_items = get_workspace_items(target_workspace)\n","\n","# Get the ID of the Source Lakehouse\n","\n","df_source_lakehouse = df_source_items[(df_source_items['displayName']==source_lakehouse) & (df_source_items['type']=='Lakehouse')]\n","source_lakehouse_id =df_source_lakehouse['id'].item()\n","\n","# Get the ID of the Target Lakehouse\n","\n","df_target_lakehouse = df_target_items[(df_target_items['displayName']==target_lakehouse) & (df_target_items['type']=='Lakehouse')]\n","target_lakehouse_id =df_target_lakehouse['id'].item()\n","\n","if len(source_lakehouse_id)<1 or len(target_lakehouse_id)<1:\n","    display(source_lakehouse_id)\n","    display(target_lakehouse_id)\n","    raise RuntimeError(\"Unable to retrieve Lakehouse IDs\")\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4213de2d-6432-4261-9dc2-ac1077e2d7a1"},{"cell_type":"markdown","source":["### Get the list of dataflows to update, using a loop to update each item"],"metadata":{},"id":"4d0a32f2-0469-4386-a5d2-0a12d2d4cc18"},{"cell_type":"code","source":["# Get the list of Dataflows in the target workspace\n","\n","#display(target_workspace_items)\n","df_update_items = df_target_items[df_target_items['type']=='Dataflow']\n","\n","display(df_update_items)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f649f65e-cae5-4d00-b502-9d821a82f730"},{"cell_type":"code","source":["items_updated = 0\n","items_skipped = 0\n","\n","for item_to_update in df_update_items.itertuples():\n","\n","    display(item_to_update.id)\n","    return_status = update_dataflow_destination(\n","        item_to_update.id,\n","        target_workspace_id # location of the items to update\n","        ,source_workspace_id,target_workspace_id\n","        ,source_lakehouse_id,target_lakehouse_id\n","        )\n","    if return_status==200:\n","        # Update the description to confirm update\n","        new_description=append_item_description('dataflows',df_update_items[\"id\"].item(),description_comment)\n","        update_item_property('dataflows',df_update_items[\"id\"].item(),'description',new_description)\n","        items_updated += 1\n","    else:\n","        items_skipped += 1"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e0a3794f-3a7a-4534-bcf6-d69007165093"},{"cell_type":"markdown","source":["### Process Complete.  Print a summary of the status and how many items were updated."],"metadata":{},"id":"eb825069-3617-4551-9c47-37ddd492f5a1"},{"cell_type":"code","source":[" # Summary of process\n","\n","print(f\"Last return status: {return_status}\")\n","print(f\"Items updated: {items_updated}\")\n","print(f\"Items skipped: {items_skipped}\")\n"," \n","mssparkutils.notebook.exit(\"Success\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd93cbd2-df0c-406d-9463-cbff7115558d"}],"metadata":{"language_info":{"name":"python"},"kernel_info":{"name":"synapse_pyspark"},"a365ComputeOptions":null,"sessionKeepAliveTimeout":0,"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":null}},"nbformat":4,"nbformat_minor":5}